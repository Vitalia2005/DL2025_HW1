# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5Qiur6MET-w7D1IwxQsdj4KO_T02M-8
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

HIDDEN_SIZE = 32
LEARNING_RATE = 0.01
EPOCHS = 10
BATCH_SIZE = 32
SEED = 42

torch.manual_seed(SEED)
np.random.seed(SEED)

def preprocess_data(df, label_encoders=None, scaler=None, is_train=True):
    categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
    numerical_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt',
                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']

    if is_train:
        label_encoders = {col: LabelEncoder().fit(df[col]) for col in categorical_cols}
        scaler = StandardScaler().fit(df[numerical_cols])

    for col in categorical_cols:
        df[col] = label_encoders[col].transform(df[col])

    df[numerical_cols] = scaler.transform(df[numerical_cols])

    return df, label_encoders, scaler, categorical_cols, numerical_cols

class LoanDataset(data.Dataset):
    def __init__(self, df, categorical_cols, numerical_cols):
        self.X_cat = df[categorical_cols].values.astype(np.int64)
        self.X_num = df[numerical_cols].values.astype(np.float32)
        self.y = df['loan_status'].values.astype(np.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return (torch.tensor(self.X_cat[idx]), torch.tensor(self.X_num[idx])), torch.tensor(self.y[idx]).unsqueeze(0)

class LoanModel(nn.Module):
    def __init__(self, input_size, cat_sizes):
        super(LoanModel, self).__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(cat_size, min(50, (cat_size + 1) // 2)) for cat_size in cat_sizes])
        self.embedding_output_size = sum([embedding.embedding_dim for embedding in self.embeddings])

        self.fc1 = nn.Linear(input_size + self.embedding_output_size, HIDDEN_SIZE)
        self.fc2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE * 4)
        self.fc3 = nn.Linear(HIDDEN_SIZE * 4, HIDDEN_SIZE)
        self.fc4 = nn.Linear(HIDDEN_SIZE, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x_cat, x_num):
        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]
        x_cat = torch.cat(x_cat, dim=1)
        x = torch.cat([x_cat, x_num], dim=1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = torch.sigmoid(self.fc4(x))
        return x.squeeze()

df = pd.read_csv("sample_data/train.csv")
df, label_encoders, scaler, categorical_cols, numerical_cols = preprocess_data(df)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=SEED)

cat_sizes = [len(label_encoders[col].classes_) for col in categorical_cols]
train_dataset = LoanDataset(train_df, categorical_cols, numerical_cols)
val_dataset = LoanDataset(val_df, categorical_cols, numerical_cols)

train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

model = LoanModel(input_size=len(numerical_cols), cat_sizes=cat_sizes)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)

def train_and_evaluate(model, train_loader, val_loader, epochs):
    train_losses, val_losses, val_aucs = [], [], []

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for (X_batch_cat, X_batch_num), y_batch in train_loader:
            optimizer.zero_grad()
            y_pred = model(X_batch_cat, X_batch_num).squeeze()  # Убираем лишнее измерение
            loss = criterion(y_pred, y_batch.squeeze())  # Убираем лишнее измерение из целевых значений
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        model.eval()
        val_loss, all_preds, all_labels = 0, [], []
        with torch.no_grad():
            for (X_batch_cat, X_batch_num), y_batch in val_loader:
                y_pred = model(X_batch_cat, X_batch_num).squeeze()  # Убираем лишнее измерение
                loss = criterion(y_pred, y_batch.squeeze())  # Убираем лишнее измерение из целевых значений
                val_loss += loss.item()
                all_preds.extend(y_pred.numpy())
                all_labels.extend(y_batch.numpy())

        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        val_auc = roc_auc_score(all_labels, all_preds)
        val_aucs.append(val_auc)

        print(f"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val ROC-AUC = {val_auc:.4f}")

    return train_losses, val_losses, val_aucs

train_losses, val_losses, val_aucs = train_and_evaluate(model, train_loader, val_loader, EPOCHS)

# Визуализация результатов
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('train loss')

plt.subplot(1,2,2)
plt.plot(range(1, EPOCHS+1), val_aucs, label='Validation ROC-AUC', color='red')
plt.xlabel('Epochs')
plt.ylabel('ROC-AUC')
plt.legend()
plt.title('ROC-AUC')

plt.show()

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('val loss')

"""Судя по графикам оптимальное количество эпох еще не достигнуто. Увелими количество эпох до 25"""

train_losses, val_losses, val_aucs = train_and_evaluate(model, train_loader, val_loader, 25)

# Визуализация результатов
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, 26), train_losses, label='Train Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('train loss')

plt.subplot(1,2,2)
plt.plot(range(1, 26), val_aucs, label='Validation ROC-AUC', color='red')
plt.xlabel('Epochs')
plt.ylabel('ROC-AUC')
plt.legend()
plt.title('ROC-AUC')

plt.show()

# Визуализация результатов
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, 26), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('val loss')

"""Судя по графикам, как раз около 10 эпохи модель начинает переобучаться. Оптимальное число эпох - примерно 10.

Поведение модели:
Train Loss постепенно снижается, что говорит о хорошем обучении модели на тренировочных данных.
Validation Loss в начале выглядит относительно стабильным, но ближе к 10-й эпохе начинает колебаться.
После 15-й эпохи наблюдаются резкие скачки вверх, что указывает на переобучение.

Validation ROC-AUC колеблется,а после 10 эпохи происходят резкие спады, что подтверждает ухудшение качества модели.

# Задача 2
"""

HIDDEN_SIZE = 128  # Увеличили hidden size
LEARNING_RATE = 0.01
EPOCHS = 10
BATCH_SIZE = 32
SEED = 42

class LoanModel(nn.Module):
    def __init__(self, input_size, cat_sizes):
        super(LoanModel, self).__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(cat_size, min(50, (cat_size + 1) // 2)) for cat_size in cat_sizes])
        self.embedding_output_size = sum([embedding.embedding_dim for embedding in self.embeddings])

        # Добавляем три блока вместо одного
        self.fc1 = nn.Linear(input_size + self.embedding_output_size, HIDDEN_SIZE)
        self.fc2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
        self.fc3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
        self.fc4 = nn.Linear(HIDDEN_SIZE, 1)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x_cat, x_num):
        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]
        x_cat = torch.cat(x_cat, dim=1)
        x = torch.cat([x_cat, x_num], dim=1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.fc4(x)
        return self.sigmoid(x).squeeze()

df = pd.read_csv("sample_data/train.csv")
df, label_encoders, scaler, categorical_cols, numerical_cols = preprocess_data(df)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=SEED)

cat_sizes = [len(label_encoders[col].classes_) for col in categorical_cols]
train_dataset = LoanDataset(train_df, categorical_cols, numerical_cols)
val_dataset = LoanDataset(val_df, categorical_cols, numerical_cols)

train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

model = LoanModel(input_size=len(numerical_cols), cat_sizes=cat_sizes)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)

train_losses, val_losses, val_aucs = train_and_evaluate(model, train_loader, val_loader, EPOCHS)

# Визуализация результатов
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('train loss')

plt.subplot(1,2,2)
plt.plot(range(1, EPOCHS+1), val_aucs, label='Validation ROC-AUC', color='red')
plt.xlabel('Epochs')
plt.ylabel('ROC-AUC')
plt.legend()
plt.title('ROC-AUC')

plt.show()

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('val loss ')

"""Loss снижается быстрее, модель лучше обучается.

# Эксперимент 3
"""

class LoanModel(nn.Module):
    def __init__(self, input_size, cat_sizes):
        super(LoanModel, self).__init__()
        self.embeddings = nn.ModuleList([nn.Embedding(cat_size, min(50, (cat_size + 1) // 2)) for cat_size in cat_sizes])
        self.embedding_output_size = sum([embedding.embedding_dim for embedding in self.embeddings])

        self.shortcut = nn.Linear(input_size + self.embedding_output_size, HIDDEN_SIZE)

        self.fc1 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
        self.bn1 = nn.BatchNorm1d(HIDDEN_SIZE, track_running_stats=False)
        self.dropout1 = nn.Dropout(0.2)

        self.fc2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
        self.bn2 = nn.BatchNorm1d(HIDDEN_SIZE, track_running_stats=False)
        self.dropout2 = nn.Dropout(0.2)

        self.fc3 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
        self.bn3 = nn.BatchNorm1d(HIDDEN_SIZE, track_running_stats=False)
        self.dropout3 = nn.Dropout(0.2)

        self.fc4 = nn.Linear(HIDDEN_SIZE, 1)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x_cat, x_num):
        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]
        x_cat = torch.cat(x_cat, dim=1)
        x = torch.cat([x_cat, x_num], dim=1)

        x = self.shortcut(x)

        residual = x
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        x = x + residual

        residual = x
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        x = x + residual

        residual = x
        x = self.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        x = x + residual

        x = self.fc4(x)
        return self.sigmoid(x).squeeze()

df = pd.read_csv("sample_data/train.csv")
df, label_encoders, scaler, categorical_cols, numerical_cols = preprocess_data(df)
train_df, val_df = train_test_split(df, test_size=0.2, random_state=SEED)

cat_sizes = [len(label_encoders[col].classes_) for col in categorical_cols]
train_dataset = LoanDataset(train_df, categorical_cols, numerical_cols)
val_dataset = LoanDataset(val_df, categorical_cols, numerical_cols)

train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

model = LoanModel(input_size=len(numerical_cols), cat_sizes=cat_sizes)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)

train_losses, val_losses, val_aucs = train_and_evaluate(model, train_loader, val_loader, EPOCHS)

# Визуализация результатов
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('train loss')

plt.subplot(1,2,2)
plt.plot(range(1, EPOCHS+1), val_aucs, label='Validation ROC-AUC', color='red')
plt.xlabel('Epochs')
plt.ylabel('ROC-AUC')
plt.legend()
plt.title('ROC-AUC')

plt.show()

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(range(1, EPOCHS+1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('val loss ')

"""Модель сходится еще быстрее, чем предудыщие, но метрики хуже"""